# ğŸš¦ **Pipeline ETL para AnÃ¡lisis de Accidentes Viales Globales**

Este proyecto proporciona un pipeline **ETL completo** para procesar y analizar datos de accidentes viales globales, utilizando datos del dataset de Kaggle ["Global Road Accidents Data"](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents) y de la **API FARS (Fatality Analysis Reporting System)**. Se emplean herramientas modernas como:

- ğŸ›°ï¸ **Kafka** para streaming de datos  
- ğŸ› ï¸ **Apache Airflow** para orquestaciÃ³n de flujos  
- ğŸ˜ **PostgreSQL** para almacenamiento  
- ğŸ³ **Docker** para contenedores  

---

## ğŸ“‹ Requisitos Previos

- ğŸ³ Docker `>= 20.10.0`  
- ğŸ“¦ Docker Compose `>= 1.29.0`  
- ğŸ§¬ Git  
- ğŸ Python `>= 3.9`  
- ğŸ““ Jupyter Notebook (para los cuadernos de anÃ¡lisis)

---

## ğŸš€ ConfiguraciÃ³n Inicial

### 1. Clonar el Repositorio

```bash
git clone https://github.com/ntlg72/ETL-Global-Road-Accidents.git
cd ETL-Global-Road-Accidents
```

### 2. Configurar Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```env
# ConfiguraciÃ³n de Postgres
PG_PASSWORD=tu_contraseÃ±a_segura
PG_DATABASE=road_accidents
PG_DATABASE_DIMENSIONAL=road_accidents_dimensional
PG_DATABASE_KAFKA=road_accidents_kafka
PG_PORT=5433

# ConfiguraciÃ³n de Airflow
AIRFLOW_UID=50000
AIRFLOW_GID=0
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
```

### 3. Estructura del Proyecto

```
docker-etl-pipeline/
â”œâ”€â”€ ETL-Global-Road-Accidents/
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ 001_extraction.ipynb
â”œâ”€â”€ consumer.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ logs/
```

---

## ğŸ§± ConstrucciÃ³n y EjecuciÃ³n de Servicios

### 4. Levantar los Servicios

```bash
docker-compose up -d --build
```

Esto iniciarÃ¡:

- ğŸ˜ PostgreSQL  
- ğŸ“¡ Zookeeper + Kafka  
- ğŸ”„ Redis (broker para Airflow)  
- ğŸŒ¬ï¸ Apache Airflow (scheduler, webserver, worker)  
- ğŸ Servicio `consumer.py`

---

## ğŸ“Š AnÃ¡lisis y ExtracciÃ³n de Datos

### 5. Ejecutar el Notebook de ExtracciÃ³n

```bash
cd ETL-Global-Road-Accidents/notebooks
pip install jupyter
jupyter notebook
```

Abre y ejecuta el cuaderno `001_extraction.ipynb`.

ğŸ” **Verifica:**
- Que Kafka y PostgreSQL estÃ©n funcionando  
- Que las conexiones a las bases de datos estÃ©n activas

---

## ğŸŒ Acceso a los Servicios

| Servicio             | URL/Puerto              | Credenciales     |
|----------------------|--------------------------|------------------|
| ğŸŒ¬ï¸ Airflow Web UI    | http://localhost:8080     | admin / admin    |
| ğŸŒ¼ Flower (Celery)   | http://localhost:5555     | -                |
| ğŸ“¡ Kafka             | localhost:9092           | -                |
| ğŸ˜ PostgreSQL        | localhost:5433           | `.env` variables |
| ğŸ Consumer API      | http://localhost:8000     | -                |

---

## ğŸ” Iniciar Proceso ETL

1. Accede a Airflow en: [http://localhost:8080](http://localhost:8080)  
2. Busca y activa el DAG `etl_accidents_pipeline`  
3. Monitorea el progreso en la interfaz de Airflow o usando:

```bash
docker logs consumer -f
```

---

## ğŸ§  CrÃ©ditos

Proyecto desarrollado por:

- ğŸ‘¨â€ğŸ’» Michel Burgos Santos  
- ğŸ‘¨â€ğŸ’» Juan David Daza Rivera  
- ğŸ‘©â€ğŸ’» Natalia LÃ³pez Gallego

---
